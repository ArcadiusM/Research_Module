\section*{Abstract}
\thispagestyle{empty}
In this paper, we examine the machine learning algorithm Random Forest
and present the results of its application.
First, we start with an explanation of the Decision Tree.
Then, we proceed to show how it can be improved.
More specifically, these improvements for the Decision Tree contain bagging, boosting and Random Forest.
There, we show that the Random Forest achieves a better prediction accuracy
by introducing randomness in tree ensembling.
This randomness provides us with a decorrelated ensemble of trees and
the increase in the number of uncorrelated trees yields lower error regarding prediction purposes. By using Random Forest, we can assess the importance of every variable
and make conclusions about data.
Moreover, we explain the intuition of Random Forest in detail including mathematical clarification.
Finally, we apply Random Forest on both simulated and real data and compare it with various methods.
In conclusion, this paper aims to introduce the relevant concepts in detail and is essentially a review. 



%As a non-parametric estimation tool, Decision Trees attract attention in the economics literature. 
%Yet, Decision Trees suffer from high variance and, 
%for prediction purposes higher variance seems to be a crucial problem, thus, 
%several improvements were proposed such as bootstrap aggregation, boosting and most importantly random forests. 
%In this project, while the main focus is being on the random forest.
%The elements of statistical learning by \cite{friedman2001elements} \cite{varian2014big} \cite{maimon2005data},
%\cite{louppe2014understanding} and as expected \cite{breiman2001random} are the main literature 
%that will be utilized in this project.

%To explain the concept of random forests in full extent, primarily Decision Trees should be discussed. 
%Exploiting the main idea and struggles with bias-variance trade-off, 
%random forests' importance can be emphasized as a more stable prediction tool \cite{maimon2005data}. 
%Conceptual comparison of random forests with bagging and boosting can deliver a better understanding of 
%its unique features as \cite{lee2019bootstrap} shows in a similar fashion. To get a further understanding, 
%random forestsâ€™ estimation process can be mathematical explained \cite{biau2012analysis} and likewise, 
%examining the consistency of estimator and showing the properties can be included \cite{breiman2004consistency}, 
%\cite{denil2014narrowing}. Also, variable importance in the tree growing process is another area that needs to be 
%delved into \cite{ishwaran2007variable} and \cite{louppe2013understanding}.
