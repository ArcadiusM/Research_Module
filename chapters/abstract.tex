\section*{Abstract}
\thispagestyle{empty}
In his paper, we examine the machine learning algorithm Random Forest 
and present the results of its application. 
First, we start with an explanation of the Decision Trees. 
Then, we proceed to show how it can be improved.
More specifically, these improvements for the Decision Trees contain bagging, boosting and Random Forest.
There, we show that the Random Forest achieves a better prediction accuracy
by introducing randomness in tree ensembling.
This randomness provides us with a decorrelated ensemble of trees and 
the increase in the number of uncorrelated trees yields lower error regarding prediction purposes. By using the Random Forest, we are able to assess the importance of every variable 
and draw conclusions about data.
Moreover, we explain the intuition of Random Forest in detail including mathematical clarification.
Finally, we apply Random Forest on both simulated and real data, and compare it with various methods. 
%Considering results, Random Forest appears to be employed in the future as well, 
%thus, a better understanding of the idea and the dynamics can give us a chance to improve. 
In conclusion, this paper aims to introduce the relevant concepts in detail and is essentially a review
and a showcase of the Random Forest algorithm. 



%As a non-parametric estimation tool, Decision Trees attract attention in the economics literature. 
%Yet, Decision Trees suffer from high variance and, 
%for prediction purposes higher variance seems to be a crucial problem, thus, 
%several improvements were proposed such as bootstrap aggregation, boosting and most importantly random forests. 
%In this project, while the main focus is being on the random forest.
%The elements of statistical learning by \cite{friedman2001elements} \cite{varian2014big} \cite{maimon2005data},
%\cite{louppe2014understanding} and as expected \cite{breiman2001random} are the main literature 
%that will be utilized in this project.

%To explain the concept of random forests in full extent, primarily Decision Trees should be discussed. 
%Exploiting the main idea and struggles with bias-variance trade-off, 
%random forests' importance can be emphasized as a more stable prediction tool \cite{maimon2005data}. 
%Conceptual comparison of random forests with bagging and boosting can deliver a better understanding of 
%its unique features as \cite{lee2019bootstrap} shows in a similar fashion. To get a further understanding, 
%random forestsâ€™ estimation process can be mathematical explained \cite{biau2012analysis} and likewise, 
%examining the consistency of estimator and showing the properties can be included \cite{breiman2004consistency}, 
%\cite{denil2014narrowing}. Also, variable importance in the tree growing process is another area that needs to be 
%delved into \cite{ishwaran2007variable} and \cite{louppe2013understanding}.
