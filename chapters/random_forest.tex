\section{Random Forest}
Decision trees, which we mentioned in the previous section, have been used for a long time. 
Deployment of decision trees is visible in simple situations and also in more complex scientific or real life and industrial affairs.
The recent popularity of decision trees is due to work presented by 
\cite{Breiman1996OUT-OF-BAG-E}, \cite{breiman2001random}, and \cite{breiman2004consistency} that ensembles of 
different decision trees can get a meaningful improvement in accuracy in classification problems and other 
common learning tasks such as regression. As the unit in this procedure is based on decision tree and includes an injection of
randomness, this method is known as a random forest. 

\subsection{Main idea and illustration}
An ensemble of randomly trained decision trees, so in other words random decision forest was defined by \cite{breiman2001random}:

\newtheorem{definition}{Definition}
\begin{definition}
	A random forest is a classifier consisting of a collection of tree-structured classifiers ${\hat{T}_{\theta_{b}}(\textbf{x})}, b = 1,...,B$ where the $\theta_{b}$ are independent identically
	distributed random vectors and each tree casts a unit vote for the most popular class at input $\textbf{x}$ .
\end{definition}

\begin{algorithm}[H]
\SetAlgoLined
\begin{enumerate}
	\item For $b$ = 1 to $B$:
	\begin{enumerate}
	    \item Draw a bootstrap sample $D_{b}$ of size N from the training data.
	    \item Grow the Random Forest tree ${{T}_{D_{b},\theta_{b}}}$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached:
	    \begin{enumerate}
	       \item Select $m$ variables denoted by $\theta_{b}$ at random from the $n$ variables
	       \item Pick the best variable/split-point among the $m$
	       \item  Split the node into two daughter nodes
	    \end{enumerate}
	\end{enumerate}
	\item  Output the ensemble of trees $\{{T}_{D_{b},\theta_{b}}\}_{b=1}^{B}$
\end{enumerate}
 \caption{Random Forest for Regression or Classification \cite{friedman2001elements}}
\end{algorithm}

\subsubsection{Randomness in the model}
The main aspect of a random decision forest model is an injection of randomness which allows to have all unit trees different from the others. Two key concepts that makes decision forest "random" are:
\begin{enumerate}
	\item Random sampling of training data points when building trees
	\item Random subsets of features considered when splitting nodes
\end{enumerate}

In practise, random sampling of training observations is done by using bootstrapping. 
Bootstrapping for random forests means that every single decision tree learns from a random sample which is drawn with replacement. 
The second important injection of randomness into model is taking into consideration only a subset of all the variables, 
when splitting each node in every unit decision tree. It means that number of variables for splitting the node has to be chosen.
 
As it is mentioned in \cite{friedman2001elements}, the inventors recommend following numbers:
\begin{enumerate}
	\item For classification:  $\lfloor{\sqrt{n}} \rfloor$ and the minimum node size is one
	\item For regression: $\lfloor \frac{n}{3} \rfloor$ and the minimum node size is five
\end{enumerate}

where $n$ is a number of features in the classification or regression problem. 
\newline
\newline 
This yields that in classification problem with 10 different variables, 
only 3 randomly chosen will be considered for splitting the node.
These numbers mentioned above are only the recommendations made by inventors and in practise the best values for these
parameters can be very different and it will depend on the problem. Therefore, these parameters
should be treated as tuning parameters. Randomness parameter, 
so the number of chosen variables has a meaningful impact on the model, 
because except controlling the amount of randomness within each tree, 
it controls also the amount of correlation between different trees in the forest. 
As randomness parameter decreases, trees become more decorrelated \cite{criminisi2012decision}.

\subsubsection {Training, testing and prediction}
Training and testing is an integral part of building up every machine learning model.
In Random forest training of all trees is done independently and testing consists in the fact that each test point $\textbf{x}$ is pushed 
through every tree included in forest until it ends in corresponding leaves. As a last step is taking all predictions from 
every unit decision tree and combining them into prediction of single random forest. Combination of different tree predictions 
can be done in different ways. As written in \cite{CGV-035}, in random forests for classification problems, forests generate probabilistic output. 
It means that they return not just a single class point prediction, but whole class distribution, 
so combination of tree predictions can be described as below:

\begin{equation}
	p(c|\textbf{x}) =  \frac{1}{B} \displaystyle\sum_{b=1}^{B} p_{b}(Y = c |X = \textbf{x})
\end{equation}
where in a random forest with the number of decision trees equals to $B$ each tth tree obtains the posterior 
distribution $ p_{b}(Y = c |X = \textbf{x}) $.

\subsubsection {Out of Bag sample}
Another important idea which is used in applications of random forest is validating model using Out of Bag sample.
When performing a bootstrap for getting a sample of data for training,
remaining part of data is our Out of Bag (OOB) sample.
After training random forest model, OOB sample will be used as not known data for prediction.
Leftover sample will be passed through every possible decision tree in the model that not include this
data in the bootstrap training sample \cite{friedman2001elements}. Out of Bag is commonly used to monitor error of predictions. 
Breimanâ€™s work \cite{Breiman1996OUT-OF-BAG-E} shows empirically that Out of Bag estimate is as accurate 
as a test set of the same size as the training set \cite{Breiman1996OUT-OF-BAG-E}.


\subsection{Mathematical explanation}

Let $D = \{(x_{1},y_{1}), (x_{2}, y_{2}), ... , (x_{N}, y_{N})\}$ be the set, we want to train our model on and $T_{D, \theta}$ 
decision tree produced by using the set $D$ and parameters $\theta$. We assume that $D$ is countable which normally is the 
case especially for Y values although replacing sums with integrals can extent the analysis and 
provide results for uncountable sets as well \cite{kohavi1996bias}. As mentioned earlier, 
Random Forest classifier selects a bootstrapped subset of observations and 
grow the decision tree with only a subset of regressors. Repeating this tree growing process $B$ times 
yields a Random Forest. Assume $x^*$ is the value that we want to predict its class, 
there are two rules that can be used to get the prediction; majority voting and 
soft voting \cite{louppe2014understanding} and \cite{zhou2012ensemble}.

In majority voting, after getting every trees prediction denoted as $\hat{T}(x^*)$ final prediction is the class that gets most votes from trees:
\begin{equation}
\boldsymbol{RF}_{D, \theta_{1}, \theta_{2}, ..., \theta_{B}} (x^*) =
	\underset{c \in Y}{argmax} \sum_{b = 1}^{B}{1(\hat{T}_{b}(x^*) = c)}
\end{equation}
In soft voting, probability estimates of a tree denoted as $\hat{p}_{D, \theta_{b}} (Y = c | X = x^*)$ is estimated and after all are averaged and most likely class is predicted:
\begin{equation}
\boldsymbol{RF}_{D, \theta_{1}, \theta_{2}, ..., \theta_{B}} (x^*) =
	\underset{c \in Y}{argmax} \dfrac{1}{B}\sum_{b = 1}^{B}{\hat{p}_{D, \theta_{b}} (Y = c | X = x^*)}
\end{equation}
As mentioned in \cite{breiman1996bagging}, the two aforementioned voting procedures provides similar results, 
yet using soft voting can provide smoother class probability estimates and be exploited in a deeper analysis setting such as 
certainty estimates investigation \cite{louppe2014understanding}. 
We introduced majority voting to enhance our understanding and will use soft voting in further derivations. 

\subsubsection{Properties}

Having explained the details of algorithm, we can explicitly show the improvement of Random Forest upon decision trees 
with exploiting the generalization error. 
We can measure of a model's fit with the generalization error which also called test error or the expected prediction error, 
we will start with examining a decision tree's generalization error and expand our finding to Random Forest. 
The derivations and findings in this section closely follow Louppe's paper \cite{louppe2014understanding}.
The generalization error of a decision tree $T_{D,\theta}$ which is grown using the set $D$ and parameters $\theta$ is;
\begin{equation}
	\boldsymbol{Err}(T_{D,\theta}) = \mathbb{E}_{X,Y}\{L(Y, T_{D,\theta}(X)) \}
\end{equation}
where $L$ is the loss function measuring the difference between its two arguments. 
Since we focus on classification setting, 
the zero-one loss function is our interest, however, 
to get a better understanding, widely used in regression type predictions, 
the squared loss function will be examined jointly. 
The bias-variance decomposition of both functions are similar and 
follow the same dynamics \cite{domingos2000decomposition}. 
While the bias-variance decomposition of zero-one loss function remains 
to be relatively unexplanatory, the squared loss provides us a superior insight. 
Therefore we denoted the findings with using the zero-one loss function with apostrophe.
The squared loss function measures the squared difference between the dependent variable and its predicted value 
by decision tree $T_{D,\theta}$ and can be defined as
\begin{equation}
	L(Y, T_{D, \theta}(x)) = (Y - T_{D, \theta}(x))^2
\end{equation}
while the zero-one loss function yields is
\begin{equation}
	L'(Y, T_{D,\theta}(X)) = \mathds{1} (Y \neq T_{D, \theta}(X))
\end{equation}
$L(Y, T_{D,\theta}(X))$ equals to $1$ if the class of dependant variable is 
different than its predicted class by decision tree and equals to 0 if both are the same, 
meaning that $Y = T_{D, \theta}(X)$. 
Both loss functions follow the same logic in essence. 
While the squared loss function yielding a number to measure the error, 
the zero-one loss function only yields $1$ or $0$.
The generalization error for both functions 
\begin{align}
	\boldsymbol{Err}(T_{D,\theta}) & = \mathbb{E}_{X,Y}\{ (Y - T_{D, \theta}(x))^2 \} \\
	\boldsymbol{Err'}(T_{D,\theta}) & = \mathbb{E}_{X,Y}\{ 1(Y \neq T_{D, \theta}(X)) \}
	= P(Y \neq T_{D, \theta}(X))
\end{align}
where the expression in the first equation measures the squared difference of true and predicted values
and the expression in the second equation is the probability of misclassification of the tree. 
We will decompose the expected generalization error and investigate the improvement of random forest upon 
decision trees.
\vspace{2mm}
\\
\textbf{\emph{The Decomposition of $\boldsymbol{Err}(T_{D,\theta})$}}\\
Given the probability distribution of P(X,Y), there exists a model $\phi_{\beta}$ that minimizes the generalization error 
and can be derived analytically independent or learning set $D$ \cite{louppe2014understanding}. 
With conditioning on X generalization error for $\phi_{\beta}$ becomes;
\begin{equation}
\mathbb{E}_{X,Y} \{L(Y, \phi_{\beta}(X))\} = \mathbb{E}_{X}\{\mathbb{E}_{Y|X}\{L(Y, \phi_{\beta}(X)) \} \}
\end{equation}
Point-wise minimization inner term  with respect to Y yields \cite{louppe2014understanding};
\begin{equation}
\phi_{\beta} = \underset{c \in Y}{argmin} \; \mathbb{E}_{Y|X=x}\{L(Y,c)\}
\end{equation}
$\phi_{\beta}$ is defined as Bayes Model and $\boldsymbol{Err}(\phi_{\beta})$ is the residual error, 
the minimum obtainable error with any model, which is considered as the irreducible error due to random deviations in the 
data\cite{louppe2014understanding}. We will exploit the irreducible concept when examining the dynamics of random forests and 
decision trees. In that sense, with couple of manipulations, Bayes Model for squared loss is
\begin{align}
\phi_{\beta} & = \underset{c \,\in\, Y}{argmin} \; \mathbb{E}_{Y|X=x}\{(Y-c)^2 \} \notag\\
			 & = \mathbb{E}_{Y|X=x}\{Y\}
\end{align}
For squared loss function, Bayes Model predicts the expected value of Y at X=x 
since the mean of $Y$ minimizes the squared difference 
and equals to its expected value.
Bayes Model of zero-one loss function denoted as $\phi_{\beta}'$ is
\begin{align}\label{eq:bayes_model}
\phi_{\beta}' & = \underset{c \,\in\, Y}{argmin} \; \mathbb{E}_{Y|X=x}\{L(Y,c) \} \notag\\
			 & = \underset{c \,\in\, Y}{argmax} \; P(Y = c \,| X = x)
\end{align}
The class with highest probability in the set $Y$ is chosen by Bayes Model when using the zero-one loss function, 
we included intermediate steps in section \ref{app:bayes_model}. 
Aforementioned residual error can be computed for both functions:
\begin{align}
\boldsymbol{Err}(\phi_{\beta}) & = \mathbb{E}_{Y|X=x}\{(Y-\phi_{\beta}(x))^2 \}\\
\boldsymbol{Err}(\phi_{\beta}') & = P(Y \neq \phi_{\beta}'(x) )
\end{align}
With using the squared loss function, $\boldsymbol{Err}(T_{D,\theta}(x))$ can be written as
\begin{align}\label{eq:decomp_squared_loss}
\boldsymbol{Err}(T_{D, \theta}(x)) & = \mathbb{E}_{Y|X=x}\{(Y-T_{D,\theta}(x))^2\} \notag \\
							   	  & = \boldsymbol{Err}(\phi_{\beta}(x)) + (\phi_{\beta}(x)-T_{D,\theta}(x))^2
\end{align}
As mentioned above, first term in the equation corresponds the 
irreducible error and the second term is due to the prediction differences between Bayes Model and our decision tree estimation. 
The generalization error increases with an increase in that difference. Since the result does not depend on the Y-values, 
it can be also expressed without conditional expectation. Decision trees in random forest classifier uses a bootstrapped dataset 
and $D$ is a random variable, thus, if we further examine the second term with taking expectation over $D$, it decomposes as
\begin{align}\label{eq:decomp_squared_loss_cont}
	& \mathbb{E}_{D}\{(\phi_{\beta}(x) - T_{D,\theta}(x))^2 \} \notag \\
	& \: = [\phi_{\beta}(x) - \mathbb{E}_{D}\{T_{D,\theta}(x)\}]^2 + 
	\mathbb{E}_{D}\{[\mathbb{E}_{D}\{T_{D,\theta}(x)\} - T_{D,\theta}(x)]^2\}
\end{align}
With intermediate steps being in section \ref{app:bias_var_decomp}, 
the first term in equation (\ref{eq:decomp_squared_loss_cont}) also called squared bias ($bias^2$) shows how the expected prediction of our 
decision tree differs from Bayes Model and the latter term is the variance of our estimator. 
Therefore, we can define $\boldsymbol{Err}(T_{D,\theta})$ as follows
\begin{equation}
\boldsymbol{Err}(T_{D,\theta}) = noise + bias^2 + var
\end{equation}
\vspace{-3mm}
\qquad \qquad \qquad \quad where
\vspace{-6.3mm}
\begin{align}
& noise = \boldsymbol{Err}(\phi_{\beta}) \notag \\
& bias^2 = [\phi_{\beta}(x) - \mathbb{E}_{D}\{T_{D,\theta}(x)\}]^2 \notag \\
& var = \mathbb{E}_{D}\{[\mathbb{E}_{D}(T_{D,\theta}(x)) - T_{D,\theta}(x)]^2\} \notag
\end{align}
The same decomposition can be conducted for the zero-one loss function in similar fashion and as mentioned in 
\cite{louppe2014understanding},\cite{domingos2000decomposition}, 
\cite{james2003variance}, \cite{friedman1997zeroLoss}
both zero-one and squared loss functions decompositions yield same results. 
However, without assuming $D$ is normally distributed, bias-variance decomposition 
cannot be solved for zero-one loss function explicitly as done for squared loss \cite{louppe2014understanding}. 
\cite{kohavi1996bias} introduces another decomposition for zero-one loss function (included in section \ref{app:kohavi_decomp}), 
but still it remains to be unexplanatory compared to squared loss, thus, 
we explain the dynamics with using squared loss although the main focus of the paper remains to be on classification setting.
\vspace{2mm}
\\
\textbf{\emph{Extending findings to Random Forest }}\\
In regression setting, random forest shares the same idea with classification prediction with soft voting. 
Random forest for regression setting can be written as
\begin{equation}
\boldsymbol{RF}_{D, \theta_{1},\theta_{2},..., \theta_{B}}(x) = \dfrac{1}{B}\sum_{b = 1}^{B}T_{D,\theta_{b}}(x)
\end{equation}
When we take the average prediction in this case equals to expectation in terms of training set, we get
\begin{align}\label{eq:mu}
\mathbb{E}_{D, \theta_{1}, \theta_{2},..., \theta_{B}}\{\boldsymbol{RF}_{D, \theta_{1},\theta_{2},..., \theta_{B}}(x) \} 
	& = \mathbb{E}_{D, \theta_{1}, \theta_{2},..., \theta_{B}}\{\dfrac{1}{B}\sum_{b = 1}^{B}T_{D,\theta_{b}}(x)\} \notag \\
	& = \dfrac{1}{B}\sum_{b=1}^{B}\mathbb{E}_{D,\theta_{b}}\{T_{D, \theta_{b}}(x)\}\notag \\
	& = \mu_{D,\theta}(x)
\end{align}
where $\mu_{D,\theta}(x)$ is the average prediction of all ensembled trees. Since $\theta$'s are random, 
independent and have the same distribution \cite{louppe2014understanding}, when we extend this finding bias of a random forest 
we can state that 
\begin{equation}\label{eq:random_forest_bias}
bias^2 = (\phi_{\beta}(x) - \mu_{D,\theta}(x))^2
\end{equation}
Intiutively, when we get the average prediction of all ensembled trees, 
we are able to consider $\mu_{D,\theta}(x)$ as one decision tree in simplest terms and make this inference.
We can interpret equation (\ref{eq:random_forest_bias}) as the squared bias cannot be decreased with ensembling randomized models, 
namely, an ensemble of trees does not guarantee having lower bias compared to only one tree \cite{friedman2001elements}.
Although random forest is inadequate to propose any structure to decrease the generalization error so far regarding $noise$ 
and $bias^2$, it displays promising performance in reducing the last remaining part of the generalization error. 
Thus, we can continue our exploration with variance of random forest, yet, we need to define the correlation coefficient $\rho(x)$
before delve into variance since the correlation coefficient have significant role in variance. 
For any two trees $T_{D,\theta'}$ and $T_{D,\theta''}$ trained with the same data $D$
and different growing parameters $\theta'$ and $\theta''$, we can define the correlation coefficient as follows
\begin{align}
	\rho(x) & 
	= \dfrac{\mathbb{E}_{D,\theta',\theta''}\{T_{D,\theta'}(x) T_{D,\theta''}(x)\} 
	- \mu_{D,\theta}^2(x)}{\sigma_{D,\theta}^2(x)}
\end{align}
We utilize the definition of the Pearson's correlation coefficient and the property of $\theta'$ and $\theta''$ following 
the same distribution in the intermediate steps in section \ref{app:corr_coef}. 
$\sigma_{D, \theta}(x)$ is the variance of a single decision tree and 
associated with prediction variability stemming from randomness of the set $D$ 
and randomness introduced with $\theta$ \cite{louppe2014understanding}.
Therefore, $\rho(x)$ represents the effect of randomization in the learning algorithm in general.
In our case, it is close to 1 when predictions of two decision trees are highly correlated and implies that randomization 
does not have a significant effect. 
On the other hand, if it is close to 0, trees are non-correlated and 
the prediction of trees are perfectly random in the sense that 
not dependent on the predictions of other trees. 
We can decompose variance of a random forest as follows:
\begin{align}\label{eq:decomp_var}
\mathbb{V}_{D, \theta_{1}, \theta_{2},..., \theta_{B}}\{\boldsymbol{RF}_{D, \theta_{1},\theta_{2},..., \theta_{B}}(x) \}  = \rho(x)\sigma^2_{D,\theta}(x) + \dfrac{1-\rho(x)}{B}\sigma^2_{D,\theta}(x)
\end{align}
We included derivations in detail in section \ref{app:var_decomp}. 
Increasing the number of ensembled trees $B$, will lower the latter expression in the equation and in the extreme 
case where $B \rightarrow \infty $, the variance of a random forest equals to $\rho(x)\sigma^2_{D,\theta}(x)$ 
and due to randomization in the algorithm $\rho(x) < 1$, 
thus, variance of a random forest is less than variance of a decision tree. 
This inference implies that the generalization error of a random forest is less than the generalization error of a decision tree. 
If the decision trees in the random forest are independent 
and consequently $\rho(x) \rightarrow 0$, the variance is reduced to $\sigma^2_{D,\theta}(x)/B$ 
which can be decreased with increasing $B$ as mentioned. 
Conclusively, Random Forest improves the performance of decision tree with decreasing variance and keeping bias unaffected.

\subsection{Interpretation}
In many cases, the main purpose of using a random forest model is its usefulness in performing predictions of a dependent variable
based on a set of explanatory variables. In addition, very often, to understand the processes under study we need to understand
which explanatory variables are the most important and useful to make mentioned predictions. 
Random Forest allows to not only build an accurate model with reliable predictions, 
but also to provide variable importance measures which are very important in the process of interpreting the model and 
its prediction results. In this section we will use \cite{louppe2013understanding} 
and \cite{gerard2016foresttour} and after general background to Mean Decrease Impurity we will examine variable importance for totally randomized tree ensembles and then we will discuss these ideas closer to the Random Forest algorithm.

\subsubsection{Variable importance}
In order to rank the importance of the explanatory variables in classification problems using random forest 
we use two possible measures. Both the first and second measure have been proposed by \cite{breiman2001random}. 
First of them is known as Mean Decrease Impurity (MDI). MDI measure is based on the total decrease in node impurity from splitting 
on the given explanatory variable and moreover MDI is averaged over all trees. 
The second measure is known as Mean Decrease Accuracy (MDA). 
This measure assumes that if the explanatory variable is not relevant to the study of the problem, 
then rearranging its values should not demolish prediction accuracy. Mean Decrease Impurity proposed 
by \cite{breiman2001random} is given by: 

\begin{equation}
	{MDI}( X_{j} ) = \frac{1}{B} \displaystyle \sum_{b=1}^{B}  \displaystyle\sum_{t \in T_{b}: v(s_{t}) 
	=  X_{j}  } p(t)\Delta i(s_{t}, t)
\end{equation}
where $ v(s_{t}) $ is the variable used in split $s_{t}$ and $ p(t) $ is the proportion $\frac{N_{t}}{N}$ of samples reaching $t$.
By definition $ MDI( X_{j} ) $ evaluates importance of a variable $ X_{j} $ for predicting $Y$ by 
adding up the weighted impurity decreases $p(t) \Delta i(s_{t}, t)$ for all nodes $t$ where $ X_{j}$ is used. 
Mean in the name of MDI comes from averaging over all $B$ trees in the random forest. 
Above definition can be used for any impurity measure $i(t)$ for example: the Gini index, the Shannon entropy. 
The second mentioned measure MDA uses out-of-bag error estimate. 
In this case to measure the importance of the $X_{j}$ variable, we have to permute its values in the out-of-bag example and 
use these all observations in the tree. $ MDA( X_{j} )$ is counted thanks to averaging the differences in 
out-of-bag error estimation after and before the permutation in all trees. Because of used permutations, 
this measure is sometimes called also as the permutation importance.

\subsubsection{Variable importance for totally randomized tree ensembles}

Let us now consider Mean Decrease Impurity as defined by equation 31 and assume a set $U$ which is a set of categorical input variables and is equal to $\{X_1,...,X_p\}$ in addition assume categorical output $Y$. For the simplicity our impurity measure which we will use is the Shannon entropy. Let us assume that we created an infinitely large ensemble of trees which are totally randomized and fully developed. Let assume also that for creation of these trees we used training sample $D$ of $N$ joint observations of $X_1,...,X_p,Y$ independently taken from the joint distribution $P(X_1,...,X_p,Y)$. A totally randomized and fully developed tree as defined in \cite{louppe2013understanding} is a decision tree with partitioned every node $t$ using a variable $X_j$ chosen uniformly and randomly among those which were not yet used at the parent nodes of $t$
and where every $t$ is split into $|\Xb_j|$ sub-trees, where $\Xb$ is an image set. For these assumptions two following theorems have been proven.

\newtheorem{theorem}{Theorem}
\begin{theorem}
    The $MDI$ of $X_j \in U$ for $Y$ as computed with an infinite ensemble of fully developed totally randomized trees and an infinitely large training sample is:
    \begin{equation}
	{MDI}( X_{j} ) = \displaystyle \sum_{k=0}^{p-1}\frac{1}{C^k_p}\frac{1}{p-k} \displaystyle\sum_{O \in \Pb_{k}(U^{-j})} I(X_j;Y|O)
    \end{equation}
    
\end{theorem}
where $U^{-j}$ denotes the subset $U-\{X_j\}$, $\Pb_{k}(U^{-j})$ is the set of subsets of $U^{-j}$
of cardinality $k$ and $I(X_j;Y|O)$ is the conditional mutual information of $X_j$ and $Y$ given the variables $O$.

\begin{theorem}
	For any ensembles of fully developed trees in asymptotic learning sample size conditions (
	e.g., in the same conditions as those of Theorem 1), we have that:
	\begin{equation}
	\displaystyle \sum_{j=1}^{p}{MDI}( X_{j} ) = I(X_1,...,X_p;Y)
     \end{equation}
	
\end{theorem}

\subsubsection{Relevant and irrelevant variable}


