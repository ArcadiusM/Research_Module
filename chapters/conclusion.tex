\section{Conclusion and outlook}

We examined and applied one of the infamous machine learning algorithms, Random Forest. 
We started with explaining the Decision Trees and the room for improvement in it. 
As solutions of high-variance in Decision Trees, bagging, boosting and Random Forest are developed. 
Random Forest shows better predictions can be achieved with introducing randomness into the picture.
That randomness provides us with a decorrelated ensemble of trees and 
the increase in the number of trees yields lower error considering prediction purposes.
With using Random Forest, we are able to assess the importance of every variable and draw conclusions about data.
We explained the intuition of Random Forest in detail and included mathematical clarification. 
Finally, we applied Random Forest on both simulated and real data and compared with various methods. 
Considering results, Random Forest appears to be employed in the future as well, 
thus, a better understanding of the idea and the dynamics can give us a chance to improve. 
This paper aims to introduce the idea in detail and essentially a review and a showcase of the Random Forest algorithm. 